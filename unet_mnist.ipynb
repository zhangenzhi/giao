{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    " \n",
    "from easydict import EasyDict as edict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# modules\n",
    "from dataloader import Cifar10DataLoader, MnistDataLoader\n",
    "from dnn import DNN\n",
    "from unet import CUNet\n",
    "from diffusion import DiffusionUnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_args = edict({\"batch_size\": 128, \"epochs\": 50, \"da\": True})\n",
    "dataloader = MnistDataLoader(dataloader_args=dataloader_args)\n",
    "train_dataset, valid_dataset, test_dataset = dataloader.load_dataset()\n",
    "\n",
    "def display(display_list):\n",
    "  plt.figure(figsize=(15, 15))\n",
    "\n",
    "  title = ['Input Image', 'Predicted Image', 'Test Image']\n",
    "\n",
    "  for i in range(len(display_list)):\n",
    "    plt.subplot(1, len(display_list), i+1)\n",
    "    plt.title(title[i])\n",
    "    plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
    "    plt.axis('off')\n",
    "  plt.show()\n",
    "\n",
    "def plot_train(display_list):\n",
    "  # plt.figure(figsize=(10, 10))\n",
    "  label = ['Train', 'Test']\n",
    "  for i in range(len(display_list)):\n",
    "    plt.plot(display_list[i], label=label[i])\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = edict({\"units\":[128,64,32,10], \"activations\":[\"relu\",\"relu\",\"relu\",\"softmax\"]})\n",
    "model = DNN(units=model_args.units, activations=model_args.activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "mt_loss_fn = tf.keras.metrics.Mean()\n",
    "test_loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "mte_loss_fn = tf.keras.metrics.Mean()\n",
    "opt_loss_fn = tf.keras.losses.categorical_crossentropy\n",
    "\n",
    "train_metrics = tf.keras.metrics.CategoricalAccuracy()\n",
    "test_metrics = tf.keras.metrics.CategoricalAccuracy()\n",
    "optimizer = tf.keras.optimizers.SGD(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function(experimental_relax_shapes=True, experimental_compile=None)\n",
    "def _train_step(inputs, labels, first_batch=False):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, feat = model(inputs)\n",
    "        loss = train_loss_fn(labels, predictions)\n",
    "        metrics = tf.reduce_mean(train_metrics(labels, predictions))\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    mt_loss_fn.update_state(loss)\n",
    "    \n",
    "    return loss, metrics, feat\n",
    "\n",
    "def _test_step(inputs, labels):\n",
    "    predictions, feat = model(inputs)\n",
    "    loss = test_loss_fn(labels, predictions)\n",
    "    opt_loss = opt_loss_fn(labels, predictions)\n",
    "    metrics = tf.reduce_mean(test_metrics(labels, predictions))\n",
    "    mte_loss_fn.update_state(loss)\n",
    "    \n",
    "    return loss, metrics, opt_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_train = iter(train_dataset)\n",
    "iter_valid = iter(valid_dataset)\n",
    "iter_test = iter(test_dataset)\n",
    "test_data =  iter_test.get_next()\n",
    "# display([test_data[\"inputs\"][0],test_data[\"inputs\"][1],test_data[\"inputs\"][2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_opt = []\n",
    "opt_label = []\n",
    "feat_const = []\n",
    "def collect_model_operator(variables, loss, feat):\n",
    "    weights = [w.numpy() for w in variables]\n",
    "    opt = DNN(units=model_args.units, \n",
    "            activations=model_args.activations,\n",
    "            init_value=weights)\n",
    "    opt_label.append(loss)\n",
    "    model_opt.append(opt)\n",
    "    feat_const.append(feat)\n",
    "\n",
    "records = edict({'epoch':[],'train_loss':[],'test_loss':[],'train_metric':[],'test_metric':[]})\n",
    "def obtain_model_opts(sample_start=30, sample_gap=20):\n",
    "    for e in range(dataloader.info.epochs):\n",
    "        mt_loss_fn.reset_states()\n",
    "        train_metrics.reset_states()\n",
    "        mte_loss_fn.reset_states()\n",
    "        test_metrics.reset_states()\n",
    "        for step in range(dataloader.info.train_step):\n",
    "            data = iter_train.get_next()\n",
    "            train_loss, acc, feat = _train_step(inputs=data[\"inputs\"], labels=data[\"labels\"])\n",
    "            if (e*dataloader.info.train_step + step)%sample_gap ==0:\n",
    "                if e >= sample_start:\n",
    "                    test_loss, test_acc, opt_loss = _test_step(inputs=test_data[\"inputs\"], labels=test_data[\"labels\"])\n",
    "                    collect_model_operator(model.trainable_variables, opt_loss, feat)\n",
    "                    \n",
    "        test_loss, test_acc, _ = _test_step(inputs=test_data[\"inputs\"], labels=test_data[\"labels\"])\n",
    "        records.epoch        += [e]\n",
    "        records.train_loss   += [mt_loss_fn.result().numpy()]\n",
    "        records.train_metric += [train_metrics.result().numpy()]\n",
    "        records.test_loss    += [mte_loss_fn.result().numpy()]\n",
    "        records.test_metric  += [test_metrics.result().numpy()]\n",
    "        log = \"\"\n",
    "        for k,v in records.items():\n",
    "            log += \"{}: {} \".format(k,v[-1])\n",
    "        print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_model_opt(raw_model_opt, data):\n",
    "    \n",
    "    def opt_test_step(opt, inputs, labels):\n",
    "        predictions, _ = opt(inputs)\n",
    "        loss = test_loss_fn(labels, predictions)\n",
    "        metrics = tf.reduce_mean(test_metrics(labels, predictions))\n",
    "        mte_loss_fn.update_state(loss)\n",
    "        return loss, metrics\n",
    "\n",
    "    for idx in range(len(raw_model_opt)):\n",
    "        mte_loss_fn.reset_states()\n",
    "        test_metrics.reset_states()\n",
    "        for step in range(1):\n",
    "            data = test_data\n",
    "            test_loss, test_acc = opt_test_step(opt=raw_model_opt[idx], inputs=data[\"inputs\"], labels=data[\"labels\"])\n",
    "        print(\"Init: opt_id:{}, Test loss:{}, Test acc:{}\".format(idx,\n",
    "                                                        mte_loss_fn.result().numpy(),\n",
    "                                                        test_metrics.result().numpy()))\n",
    "            \n",
    "def hard_save_model_opt(online_model_opt, path=\"./model_opt\"):\n",
    "    labels = [lab.numpy() for lab in opt_label]\n",
    "    lab_path = os.path.join(path, \"lab\")\n",
    "    np.save(lab_path, np.asarray(labels))\n",
    "    \n",
    "    init_model_opt(online_model_opt, test_data)\n",
    "    for idx in range(len(online_model_opt)):\n",
    "        mpath = os.path.join(path, \"opt_{}\".format(idx))\n",
    "        online_model_opt[idx].save(mpath, overwrite=True, save_format='tf')\n",
    "\n",
    "def load_model_opt(path=\"./model_opt\"):\n",
    "    offline_model_opt = []\n",
    "    model_opt_list = os.listdir(path=path)\n",
    "    for idx in range(len(model_opt_list)-1):\n",
    "        mpath = os.path.join(path,  \"opt_{}\".format(idx))\n",
    "        offline_model_opt.append(tf.keras.models.load_model(mpath))\n",
    "    init_model_opt(offline_model_opt, test_data)\n",
    "    \n",
    "    labels = np.load(path+\"/lab.npy\")\n",
    "    label_opts = [tf.constant(lab) for lab in list(labels)] \n",
    "    \n",
    "    return offline_model_opt, label_opts\n",
    "\n",
    "def sample_from_dict(test_label, sample_pool):\n",
    "    samples = []\n",
    "    labels = test_label\n",
    "    for lab in labels:\n",
    "        key = str(np.argmax(lab, axis=0))\n",
    "        choice_idx = random.sample(range(len(sample_pool[key])), 1)[0]\n",
    "        samples += [sample_pool[key][choice_idx]]\n",
    "    da = tf.keras.Sequential([preprocessing.Resizing(32,32)])\n",
    "    da_sample = da(tf.constant(samples))\n",
    "    sample_dict = {\"inputs\":da_sample, \"labels\":test_label}\n",
    "    return sample_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "# obtain_model_opts(sample_start=30, sample_gap=20) \n",
    "# print(len(model_opt))\n",
    "\n",
    "# load model opt\n",
    "model_opt, opt_label = load_model_opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GIAO_EPOCH = 11\n",
    "GIAO_BATCH = 1\n",
    "\n",
    "giao_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "giao_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "def _opt_train_step(unet, regs, train_inputs, train_labels, labels):\n",
    "    gradients = []\n",
    "    losses = []\n",
    "    for idx in range(len(regs)):\n",
    "        with tf.GradientTape() as tape:\n",
    "            pseudo_inputs = unet(train_inputs)\n",
    "            predictions, feat = regs[idx](pseudo_inputs)\n",
    "            reg_loss = opt_loss_fn(train_labels, predictions)\n",
    "            giao_loss = tf.reduce_mean(tf.abs(labels[idx]-reg_loss))\n",
    "        \n",
    "            losses.append(giao_loss)\n",
    "            # print(giao_loss)\n",
    "            grad = tape.gradient(giao_loss, unet.model.trainable_variables)\n",
    "            if gradients == []:\n",
    "                gradients = grad\n",
    "            else:\n",
    "                gradients = [sg1+sg2 for sg1,sg2 in zip(grad, gradients)]\n",
    "        \n",
    "    reduced_grads = [g/GIAO_BATCH for g in gradients]\n",
    "    giao_optimizer.apply_gradients(zip(reduced_grads, unet.model.trainable_variables))\n",
    "    reduced_loss = sum(losses)/GIAO_BATCH\n",
    "    return reduced_loss, pseudo_inputs\n",
    "\n",
    "# def _opt_train_step(unet, regs, train_inputs, train_labels, labels):\n",
    "#     gradients = []\n",
    "#     losses = []\n",
    "#     for idx in range(len(regs)):\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             pseudo_inputs = unet(train_inputs)\n",
    "#             # pseudo_inputs = train_inputs\n",
    "#             _, feat_reg = regs[idx](train_inputs)\n",
    "#             predictions, feat = regs[idx](pseudo_inputs)\n",
    "#             reg_loss = opt_loss_fn(train_labels, predictions)\n",
    "#             # print(reg_loss, labels[idx])\n",
    "#             giao_loss = giao_loss_fn(labels[idx], reg_loss)\n",
    "#             feat_loss = tf.norm(feat-feat_reg)/128\n",
    "#             # giao_loss = tf.math.reduce_sum(labels[idx]-reg_loss)\n",
    "#             losses.append(giao_loss+feat_loss)\n",
    "#             # print(giao_loss)\n",
    "#             grad = tape.gradient(giao_loss+feat_loss, unet.model.trainable_variables)\n",
    "#             if gradients == []:\n",
    "#                 gradients = grad\n",
    "#             else:\n",
    "#                 gradients = [sg1+sg2 for sg1,sg2 in zip(grad, gradients)]\n",
    "        \n",
    "#     reduced_grads = [g/GIAO_BATCH for g in gradients]\n",
    "#     giao_optimizer.apply_gradients(zip(reduced_grads, unet.model.trainable_variables))\n",
    "#     reduced_loss = sum(losses)/GIAO_BATCH\n",
    "#     return reduced_loss, pseudo_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet = UNet(input_shape=[32, 32, 3])\n",
    "# unet = CUNet(input_shape=[32, 32, 1])\n",
    "unet = DiffusionUnet(img_channels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pool,_ = dataloader.load_datadict()\n",
    "\n",
    "for j in range(5000):\n",
    "    train_data = sample_from_dict(test_label=test_data[\"labels\"],sample_pool=sample_pool)\n",
    "    pseudo_inputs = train_data[\"inputs\"]\n",
    "    for i in range(GIAO_EPOCH):\n",
    "        idx = random.sample(range(len(model_opt)), GIAO_BATCH)\n",
    "        labels = [opt_label[i] for i in idx]\n",
    "        regs = [model_opt[i] for i in idx]\n",
    "        giao_train_loss, pseudo_inputs = _opt_train_step(unet, regs, pseudo_inputs, test_data[\"labels\"], labels)\n",
    "        if (j*GIAO_EPOCH+ i) % 10 == 0:\n",
    "            print(\"Epoch:{}, GIAO_Train_Loss:{}\".format(j, giao_train_loss))\n",
    "            display([train_data[\"inputs\"].numpy()[0], pseudo_inputs.numpy()[0], test_data[\"inputs\"][0]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code test\n",
    "for i in range(10):\n",
    "    display([train_data[\"inputs\"].numpy()[i], pseudo_inputs.numpy()[i], test_data[\"inputs\"].numpy()[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model opt\n",
    "hard_save_model_opt(model_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model opt\n",
    "model_opt, opt_label = load_model_opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics visualization\n",
    "plot_train([records.train_loss, records.test_loss])\n",
    "plot_train([opt_label, opt_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = iter_train.get_next()\n",
    "\n",
    "unet = UNet(input_shape=[32, 32, 3])\n",
    "output = unet(train_data[\"inputs\"])\n",
    "print(output.shape)\n",
    "display([train_data[\"inputs\"].numpy()[0],output.numpy()[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
