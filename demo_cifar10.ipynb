{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from easydict import EasyDict as edict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# modules\n",
    "from dataloader import Cifar10DataLoader, MnistDataLoader\n",
    "from dnn import DNN\n",
    "from unet import UNet,CUNet\n",
    "from diffusion import DiffusionUnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_args = edict({\"batch_size\": 256, \"epochs\": 50, \"da\": False})\n",
    "# dataloader = MnistDataLoader(dataloader_args=dataloader_args)\n",
    "dataloader = Cifar10DataLoader(dataloader_args=dataloader_args)\n",
    "train_dataset, valid_dataset, test_dataset = dataloader.load_dataset()\n",
    "\n",
    "def display(display_list):\n",
    "  plt.figure(figsize=(15, 15))\n",
    "\n",
    "  title = ['Input Image', 'Predicted Image', 'Test Image']\n",
    "\n",
    "  for i in range(len(display_list)):\n",
    "    plt.subplot(1, len(display_list), i+1)\n",
    "    plt.title(title[i])\n",
    "    plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
    "    plt.axis('off')\n",
    "  plt.show()\n",
    "\n",
    "def plot_train(display_list):\n",
    "  # plt.figure(figsize=(10, 10))\n",
    "  label = ['Train', 'Test']\n",
    "  for i in range(len(display_list)):\n",
    "    plt.plot(display_list[i], label=label[i])\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = edict({\"units\":[128,64,32,10], \"activations\":[\"relu\",\"relu\",\"relu\",\"softmax\"]})\n",
    "model = DNN(units=model_args.units, activations=model_args.activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "mt_loss_fn = tf.keras.metrics.Mean()\n",
    "test_loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "mte_loss_fn = tf.keras.metrics.Mean()\n",
    "opt_loss_fn = tf.keras.losses.categorical_crossentropy\n",
    "pat_loss_fn = tf.keras.losses.mean_squared_error\n",
    "\n",
    "train_metrics = tf.keras.metrics.CategoricalAccuracy()\n",
    "test_metrics = tf.keras.metrics.CategoricalAccuracy()\n",
    "optimizer = tf.keras.optimizers.SGD(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function(experimental_relax_shapes=True, experimental_compile=None)\n",
    "def _train_step(inputs, labels, first_batch=False):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, pattern = model(inputs)\n",
    "        loss = train_loss_fn(labels, predictions)\n",
    "        metrics = tf.reduce_mean(train_metrics(labels, predictions))\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    mt_loss_fn.update_state(loss)\n",
    "    \n",
    "    return loss, metrics, pattern\n",
    "\n",
    "def _test_step(inputs, labels):\n",
    "    predictions = model(inputs)\n",
    "    loss = test_loss_fn(labels, predictions)\n",
    "    opt_loss = opt_loss_fn(labels, predictions)\n",
    "    metrics = tf.reduce_mean(test_metrics(labels, predictions))\n",
    "    mte_loss_fn.update_state(loss)\n",
    "    \n",
    "    return loss, metrics, opt_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_train = iter(train_dataset)\n",
    "iter_valid = iter(valid_dataset)\n",
    "iter_test = iter(test_dataset)\n",
    "test_data =  iter_test.get_next()\n",
    "\n",
    "display([test_data[\"inputs\"][0],test_data[\"inputs\"][1],test_data[\"inputs\"][2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_opt = []\n",
    "opt_label = []\n",
    "mid_pattern = []\n",
    "def collect_model_operator(variables, loss, pattern):\n",
    "    weights = [w.numpy() for w in variables]\n",
    "    opt = DNN(units=model_args.units, \n",
    "            activations=model_args.activations,\n",
    "            init_value=weights)\n",
    "    opt_label.append(loss)\n",
    "    mid_pattern.append(pattern)\n",
    "    model_opt.append(opt)\n",
    "\n",
    "records = edict({'epoch':[],'train_loss':[],'test_loss':[],'train_metric':[],'test_metric':[]})\n",
    "def obtain_model_opts(sample_start=30, sample_gap=20):\n",
    "    for e in range(dataloader.info.epochs):\n",
    "        mt_loss_fn.reset_states()\n",
    "        train_metrics.reset_states()\n",
    "        mte_loss_fn.reset_states()\n",
    "        test_metrics.reset_states()\n",
    "        for step in range(dataloader.info.train_step):\n",
    "            data = iter_train.get_next()\n",
    "            train_loss, acc, partern = _train_step(inputs=data[\"inputs\"], labels=data[\"labels\"])\n",
    "            if (e*dataloader.info.train_step + step)%sample_gap ==0:\n",
    "                if e >= sample_start:\n",
    "                    test_loss, test_acc, opt_loss = _test_step(inputs=test_data[\"inputs\"], labels=test_data[\"labels\"])\n",
    "                    collect_model_operator(model.trainable_variables, opt_loss, partern)\n",
    "                    \n",
    "        test_loss, test_acc, _ = _test_step(inputs=test_data[\"inputs\"], labels=test_data[\"labels\"])\n",
    "        records.epoch        += [e]\n",
    "        records.train_loss   += [mt_loss_fn.result().numpy()]\n",
    "        records.train_metric += [train_metrics.result().numpy()]\n",
    "        records.test_loss    += [mte_loss_fn.result().numpy()]\n",
    "        records.test_metric  += [test_metrics.result().numpy()]\n",
    "        log = \"\"\n",
    "        for k,v in records.items():\n",
    "            log += \"{}: {} \".format(k,v[-1])\n",
    "        print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_model_opt(raw_model_opt, data):\n",
    "    \n",
    "    def opt_test_step(opt, inputs, labels):\n",
    "        predictions = opt(inputs)\n",
    "        loss = test_loss_fn(labels, predictions)\n",
    "        metrics = tf.reduce_mean(test_metrics(labels, predictions))\n",
    "        mte_loss_fn.update_state(loss)\n",
    "        return loss, metrics\n",
    "\n",
    "    for idx in range(len(raw_model_opt)):\n",
    "        mte_loss_fn.reset_states()\n",
    "        test_metrics.reset_states()\n",
    "        for step in range(1):\n",
    "            data = test_data\n",
    "            test_loss, test_acc = opt_test_step(opt=raw_model_opt[idx], inputs=data[\"inputs\"], labels=data[\"labels\"])\n",
    "        print(\"Init: opt_id:{}, Test loss:{}, Test acc:{}\".format(idx,\n",
    "                                                        mte_loss_fn.result().numpy(),\n",
    "                                                        test_metrics.result().numpy()))\n",
    "            \n",
    "def hard_save_model_opt(online_model_opt, path=\"./model_opt\"):\n",
    "    init_model_opt(online_model_opt, test_data)\n",
    "    for idx in range(len(online_model_opt)):\n",
    "        mpath = os.path.join(path, \"opt_{}\".format(idx))\n",
    "        online_model_opt[idx].save(mpath, overwrite=True, save_format='tf')\n",
    "\n",
    "def load_model_opt(path=\"./model_opt\"):\n",
    "    offline_model_opt = []\n",
    "    model_opt_list = os.listdir(path=path)\n",
    "    for idx in range(len(model_opt_list)):\n",
    "        mpath = os.path.join(path,  \"opt_{}\".format(idx))\n",
    "        offline_model_opt.append(tf.keras.models.load_model(mpath))\n",
    "    init_model_opt(offline_model_opt, test_data)\n",
    "    return offline_model_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GIAO_EPOCH = 1000\n",
    "GIAO_BATCH = 1\n",
    "\n",
    "giao_optimizer = tf.keras.optimizers.Adam(2e-4)\n",
    "giao_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "def _opt_train_step(unet, regs, train_inputs, train_labels, optls, pat):\n",
    "    gradients = []\n",
    "    losses = []\n",
    "    for idx in range(len(regs)):\n",
    "        with tf.GradientTape() as tape:\n",
    "            pseudo_inputs = unet(train_inputs)\n",
    "            # pseudo_inputs = train_inputs\n",
    "            predictions, ppat = regs[idx](pseudo_inputs)\n",
    "            pat_loss = pat_loss_fn(ppat, pat)\n",
    "            reg_loss = opt_loss_fn(train_labels, predictions)\n",
    "            # print(reg_loss, labels[idx])\n",
    "            giao_loss = giao_loss_fn(optls[idx], reg_loss) + pat_loss\n",
    "            # giao_loss = tf.math.reduce_sum(labels[idx]-reg_loss)\n",
    "            losses.append(giao_loss)\n",
    "            # print(giao_loss)\n",
    "            grad = tape.gradient(giao_loss, unet.model.trainable_variables)\n",
    "            if gradients == []:\n",
    "                gradients = grad\n",
    "            else:\n",
    "                gradients = [sg1+sg2 for sg1,sg2 in zip(grad, gradients)]\n",
    "        \n",
    "    reduced_grads = [g/GIAO_BATCH for g in gradients]\n",
    "    giao_optimizer.apply_gradients(zip(reduced_grads, unet.model.trainable_variables))\n",
    "    reduced_loss = sum(losses)/GIAO_BATCH\n",
    "    return reduced_loss, pseudo_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = iter_train.get_next()\n",
    "\n",
    "# unet = UNet(input_shape=[32, 32, 3])\n",
    "unet = CUNet(input_shape=[32, 32, 3])\n",
    "# unet = DiffusionUnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(GIAO_EPOCH):\n",
    "    idx = random.sample(range(len(model_opt)), GIAO_BATCH)\n",
    "    labels = [opt_label[i] for i in idx]\n",
    "    regs = [model_opt[i] for i in idx]\n",
    "    giao_train_loss, pseudo_inputs = _opt_train_step(unet, regs, test_data[\"inputs\"], test_data[\"labels\"], labels)\n",
    "    if i % 25 == 0:\n",
    "        print(\"Epoch:{} GIAO Train Loss:{}\".format(i, giao_train_loss))\n",
    "        display([test_data[\"inputs\"].numpy()[0], pseudo_inputs.numpy()[0], test_data[\"inputs\"][0]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code test\n",
    "for i in range(3):\n",
    "    display([train_data[\"inputs\"].numpy()[i], pseudo_inputs.numpy()[i], test_data[\"inputs\"].numpy()[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "obtain_model_opts(sample_start=30, sample_gap=5) \n",
    "print(len(model_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model opt\n",
    "hard_save_model_opt(model_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model opt\n",
    "model_opt = load_model_opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics visualization\n",
    "plot_train([records.train_loss, records.test_loss])\n",
    "plot_train([opt_label, opt_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = iter_train.get_next()\n",
    "\n",
    "unet = UNet(input_shape=[32, 32, 3])\n",
    "output = unet(train_data[\"inputs\"])\n",
    "print(output.shape)\n",
    "display([train_data[\"inputs\"].numpy()[0],output.numpy()[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Jun  1 2022, 06:34:44) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14ddbbabdf1cc89aed24e001be3922f4034073f682acd1338a644a14376bd924"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
